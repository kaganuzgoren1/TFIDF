{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TFIDF\n",
        "The TF-IDF(Term-Frequency- Inverse Document Frequency) is the statistics-based method's baseline for finding the keywords. TF, the term-frequency, is the ratio between the frequency of a term and the document's total number of words[1]. The IDF (Inverse Document Frequency) is the representation of the entire document's logarithm based on the corpus and the ratio of the documents having the term in it [1]. The result obtained after the multiplication of the two values is the TF-IDF score [2]. The formula for TF-IDF is:\n",
        "\n",
        "$ w _(t _k)=tf_k*log \\frac{N}{df_k}\\$\n",
        "\n",
        "where $tf_k$ represents the TF of $t_k$,   $ log \\frac{N}{df_k}\\ $ represents the IDF of  $t_k$ and $w_(t_k)$ is the TF-IDF weight [2]. The TF-IDF aims to show that for a term with a higher frequency in a document if there are fewer documents in the corpus containing it inside, it is more likely a representative word for the particular document [2].  \n",
        "\n",
        "[1] Liu, Xingbing, et al. \"Keywords extraction method for technological demands of small and medium-sized enterprises based on LDA.\" 2019 Chinese Automation Congress (CAC). IEEE, 2019.\n",
        "\n",
        "[2] Chen, Kewen, et al. \"Turning from TF-IDF to TF-IGM for term weighting in text classification.\" Expert Systems with Applications 66 (2016): 245-260.\n"
      ],
      "metadata": {
        "id": "QhC2QNkz7dH0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF\n",
        "In this assessment, TfidfVectorizer from the sklearn library is used. It converts a collection of raw documents to a matrix of TF-IDF features. It is the way to use TfidfTransformer after CountVectorizer. CountVectorizer converts a collection of text documents to a matrix of token counts. TfidfTransformer does the TF-IDF transformation from a provided matrix of counts. It is important to mention that for TfidfVectorizer, the default \"smooth_idf\" feature is True. So, it weights by adding one to document frequencies, which prevents zero divisions. Furthermore, stop words, which include \"and,\" \"the,\" and \"him,\" are seen to be uninformative in describing the content of a document. They are eliminated to prevent them from being interpreted as a signal for prediction. "
      ],
      "metadata": {
        "id": "HIA22K2t9hG7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pandas and scikit-learn libraries are downloaded\n",
        "!pip install scikit-learn\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MeScf-nNmhW1",
        "outputId": "67e71ffa-b0f0-4915-ca29-190c6d365043"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.7.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2022.2.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#Train data and Test data can be anything that has sentences as strings inside. For this experiment, random sentences, including Wikipedia data, are chosen.\n",
        "#Two different Test datasets are used. This shows the different values observed after changing the datasets. The TFIDF values and the terms are changed regarding the test and train datasets.\n",
        "#The first test dataset is a piece of the training dataset.\n",
        "\n",
        "train = [\n",
        "    \"I enjoy reading about Machine Learning and it is my PhD subject.\",\n",
        "    \"I would enjoy a walk in the park.\",\n",
        "    \"I was reading in the library.\",\n",
        "    \"You can not leave the library.\",\n",
        "    \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. \",\n",
        "    \"Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\"\n",
        "\n",
        "]\n",
        "test = [\n",
        "        \"I enjoy reading about Machine Learning and it is my PhD subject.\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "kd9-BzvvDSVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train data and Test data can be anything that has sentences as strings inside. For this experiment, random sentences including Wikipedia data are chosen.\n",
        "#Two different Train and Test datasets are used. This shows the different values observed after changing the datasets. The TFIDF values and the terms are changed regarding to the test and train datasets.\n",
        "# The second test dataset does not belong to the training dataset. With the created TF-IDF class, even though new text data that is different than the training data arrives, if there are words existing in both of the datasets, the TF-IDF values are found. \n",
        "#The results are situated below.\n",
        "\n",
        "train= [\n",
        "    \"I enjoy reading about machine Learning and it is my PhD subject.\",\n",
        "    \"I would enjoy a walk in the park.\",\n",
        "    \"I was reading in the library.\",\n",
        "    \"You can not leave the library.\",\n",
        "    \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. \",\n",
        "    \"Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\"\n",
        "\n",
        "]\n",
        "test_1 = [\n",
        "    \"It is a big park. After walking some time in the park, I will go to the library to study Machine Learning. I have to finish my reading before leaving. \"\n",
        "]"
      ],
      "metadata": {
        "id": "C06tyBN87ZHG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class tfidfku():\n",
        "  \"\"\"\n",
        "  This class has two inputs as train and test. Train symbolizes the training dataset and test symbolizes the test dataset.\n",
        "  The TfidfVectorizer of Sklearn library that also implements english stop word removal is used. This class helps the user \n",
        "  to find the TF-IDF values of the terms existing in the dataset regarding to the vocabulary existing in the training set. \n",
        "  Finally, it creates a Pandas DataFrame that illustrates the correponding TF-IDF value for the term.\n",
        "  \n",
        "  Functions:\n",
        "  process()\n",
        "  dfresults()\n",
        "  showresults()\n",
        "\n",
        "\n",
        "  \"\"\"\n",
        "  def __init__(self,train=train,test=test) :\n",
        "    \"\"\"\n",
        "    Parameters: \n",
        "    train: training dataset that is a list of sentences.(string)\n",
        "    test: test dataset that is a list of sentences.(string)\n",
        "    tfidfvectorizer: the sklearn library's TfidfVectorizer with stop word removal included.\n",
        "\n",
        "    \"\"\"\n",
        "    self.train=train\n",
        "    self.test=test\n",
        "    self.tfidfvectorizer = TfidfVectorizer(stop_words='english',smooth_idf=True)\n",
        "    self.process()\n",
        "    self.dfresults()\n",
        "  def process(self):\n",
        "    \"\"\"\n",
        "    process: tfidfvectorizer fitted to the training dataset with .fit function. It learns vocabulary and the idf from the training set. \n",
        "    Then, regarding to the learned vocabulary, by using .transform function, the document-term matrix (tfidf_term_vectors)) of the test dataset is obtained.\n",
        "\n",
        "    \"\"\"\n",
        "    self.tfidfvectorizer.fit(self.train)\n",
        "    self.tfidf_term_vectors  = self.tfidfvectorizer.transform(self.test)\n",
        "  def dfresults(self):\n",
        "    \"\"\"\n",
        "    dfresults: A pandas dataframe that illustrates the TF-IDF values of corresponding word is created.\n",
        "\n",
        "    \"\"\"\n",
        "    self.df = pd.DataFrame(self.tfidf_term_vectors[0].T.todense(), index=self.tfidfvectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
        "    self.df = self.df.sort_values('TF-IDF', ascending=False)\n",
        "  def showresults(self):\n",
        "    \"\"\"\n",
        "    showresults: It shows the DataFrame created by dfresults function.\n",
        "    \n",
        "    \"\"\"\n",
        "    return self.df\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "YvsRCoQhplln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl = tfidfku(train,test)"
      ],
      "metadata": {
        "id": "Iy0yhrEypmmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl.showresults()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pafXHjyoptQL",
        "outputId": "044c3eca-4f4c-4999-af17-8a29756a1c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 TF-IDF\n",
              "subject        0.482050\n",
              "phd            0.482050\n",
              "enjoy          0.395288\n",
              "reading        0.395288\n",
              "machine        0.333729\n",
              "learning       0.333729\n",
              "algorithms     0.000000\n",
              "recognition    0.000000\n",
              "model          0.000000\n",
              "needed         0.000000\n",
              "order          0.000000\n",
              "park           0.000000\n",
              "perform        0.000000\n",
              "performance    0.000000\n",
              "predictions    0.000000\n",
              "programmed     0.000000\n",
              "sample         0.000000\n",
              "methods        0.000000\n",
              "seen           0.000000\n",
              "set            0.000000\n",
              "speech         0.000000\n",
              "tasks          0.000000\n",
              "training       0.000000\n",
              "understanding  0.000000\n",
              "unfeasible     0.000000\n",
              "used           0.000000\n",
              "variety        0.000000\n",
              "vision         0.000000\n",
              "walk           0.000000\n",
              "ml             0.000000\n",
              "make           0.000000\n",
              "medicine       0.000000\n",
              "applications   0.000000\n",
              "artificial     0.000000\n",
              "based          0.000000\n",
              "build          0.000000\n",
              "building       0.000000\n",
              "computer       0.000000\n",
              "conventional   0.000000\n",
              "data           0.000000\n",
              "decisions      0.000000\n",
              "develop        0.000000\n",
              "devoted        0.000000\n",
              "difficult      0.000000\n",
              "email          0.000000\n",
              "explicitly     0.000000\n",
              "field          0.000000\n",
              "filtering      0.000000\n",
              "improve        0.000000\n",
              "inquiry        0.000000\n",
              "intelligence   0.000000\n",
              "known          0.000000\n",
              "learn          0.000000\n",
              "leave          0.000000\n",
              "leverage       0.000000\n",
              "library        0.000000\n",
              "wide           0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18d6c852-8e52-4cbd-bc3b-543a7ee4bf6b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>0.482050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phd</th>\n",
              "      <td>0.482050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>0.395288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reading</th>\n",
              "      <td>0.395288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>machine</th>\n",
              "      <td>0.333729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.333729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algorithms</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recognition</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>needed</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>park</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perform</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>performance</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>predictions</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>programmed</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>methods</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seen</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speech</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tasks</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>understanding</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unfeasible</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>used</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variety</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walk</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ml</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medicine</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>applications</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artificial</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>based</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>build</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>building</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computer</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conventional</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decisions</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>develop</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>devoted</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>explicitly</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>field</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>filtering</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>improve</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inquiry</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>intelligence</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>known</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learn</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leave</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leverage</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>library</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wide</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18d6c852-8e52-4cbd-bc3b-543a7ee4bf6b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18d6c852-8e52-4cbd-bc3b-543a7ee4bf6b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18d6c852-8e52-4cbd-bc3b-543a7ee4bf6b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 274
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cl2 = tfidfku(train,test_1)"
      ],
      "metadata": {
        "id": "59u1msc78F7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cl2.showresults()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O2Hhlimk8GIk",
        "outputId": "f1e16d83-d55b-4093-bd6b-6ebfd68401f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 TF-IDF\n",
              "park           0.796602\n",
              "library        0.326612\n",
              "reading        0.326612\n",
              "machine        0.275749\n",
              "learning       0.275749\n",
              "algorithms     0.000000\n",
              "methods        0.000000\n",
              "model          0.000000\n",
              "needed         0.000000\n",
              "order          0.000000\n",
              "perform        0.000000\n",
              "performance    0.000000\n",
              "phd            0.000000\n",
              "predictions    0.000000\n",
              "programmed     0.000000\n",
              "recognition    0.000000\n",
              "sample         0.000000\n",
              "seen           0.000000\n",
              "set            0.000000\n",
              "speech         0.000000\n",
              "subject        0.000000\n",
              "tasks          0.000000\n",
              "training       0.000000\n",
              "understanding  0.000000\n",
              "unfeasible     0.000000\n",
              "used           0.000000\n",
              "variety        0.000000\n",
              "vision         0.000000\n",
              "walk           0.000000\n",
              "ml             0.000000\n",
              "make           0.000000\n",
              "medicine       0.000000\n",
              "applications   0.000000\n",
              "artificial     0.000000\n",
              "based          0.000000\n",
              "build          0.000000\n",
              "building       0.000000\n",
              "computer       0.000000\n",
              "conventional   0.000000\n",
              "data           0.000000\n",
              "decisions      0.000000\n",
              "develop        0.000000\n",
              "devoted        0.000000\n",
              "difficult      0.000000\n",
              "email          0.000000\n",
              "enjoy          0.000000\n",
              "explicitly     0.000000\n",
              "field          0.000000\n",
              "filtering      0.000000\n",
              "improve        0.000000\n",
              "inquiry        0.000000\n",
              "intelligence   0.000000\n",
              "known          0.000000\n",
              "learn          0.000000\n",
              "leave          0.000000\n",
              "leverage       0.000000\n",
              "wide           0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-06e8bcad-0638-4d8f-a43e-34d781effcb3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>park</th>\n",
              "      <td>0.796602</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>library</th>\n",
              "      <td>0.326612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>reading</th>\n",
              "      <td>0.326612</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>machine</th>\n",
              "      <td>0.275749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.275749</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algorithms</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>methods</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>needed</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perform</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>performance</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phd</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>predictions</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>programmed</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recognition</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>seen</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speech</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tasks</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>understanding</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unfeasible</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>used</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variety</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walk</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ml</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medicine</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>applications</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artificial</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>based</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>build</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>building</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computer</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conventional</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>data</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decisions</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>develop</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>devoted</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>explicitly</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>field</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>filtering</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>improve</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inquiry</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>intelligence</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>known</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learn</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leave</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leverage</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wide</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06e8bcad-0638-4d8f-a43e-34d781effcb3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-06e8bcad-0638-4d8f-a43e-34d781effcb3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-06e8bcad-0638-4d8f-a43e-34d781effcb3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unit Test\n",
        "\n",
        "Unit tests are used to see if some errors can appear during the experiment. In Edge cases, it is more likely to see the errors. They refer to the beginning and the end of the program. So it is essential to examine the tests specifically in these areas. The Unit Tests created aim to handle the Edge Cases."
      ],
      "metadata": {
        "id": "dwP_72dnrg-O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenizer and English stop words are installed to be used in the Unit Tests.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bne1twZKx8IZ",
        "outputId": "2c8ed0c3-addd-4d67-88eb-3926dd63e4da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class tfidftest(unittest.TestCase):\n",
        "\n",
        "    \"\"\"\n",
        "    This is the class created for Unit Testing for the TFIDF class (tfidfku) shown above. \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        In the setUp part, tfidfku class with the train set as \"train\" and test set as \"test\" is created. \n",
        "        The tfidfku class also creates pandas DF thanks to its architecture.\n",
        "        Parameters:\n",
        "        tokenizedtrain,tokenizedtest : word tokenized form of the train and test datasets, respectively.\n",
        "\n",
        "        \"\"\"\n",
        "        self.cltfidf=tfidfku(train,test)\n",
        "        self.tokenizedtrain=[]\n",
        "        for sents in train:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              self.tokenizedtrain.append(word_tokenize(sentences))\n",
        "        self.tokenizedtest=[]\n",
        "        for sents in test:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              self.tokenizedtest.append(word_tokenize(sentences))\n",
        "\n",
        "        \n",
        "\n",
        "    def test_trainstr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set.\n",
        "        \"\"\"\n",
        "        \n",
        "        for lines in self.cltfidf.train:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_teststr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set.\n",
        "\n",
        "        \"\"\"\n",
        "        for lines in self.cltfidf.test:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_stopwordsnotexist(self):\n",
        "        \"\"\"\n",
        "        As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. \n",
        "        \n",
        "        \"\"\"\n",
        "        for words in stopwords.words(\"english\"):\n",
        "            self.assertNotIn(words,self.cltfidf.df.index)\n",
        "    \n",
        "    def test_similar(self):\n",
        "        \"\"\"\n",
        "        In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone.\n",
        "        So, it is expected that they have equal TF-IDF values. \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.assertAlmostEqual(self.cltfidf.df.loc[\"machine\"][0],self.cltfidf.df.loc[\"learning\"][0])\n",
        "\n",
        "    def test_equalzero(self):\n",
        "        \"\"\"\n",
        "        A non used word in the test set is expected to have a 0 as the TF-IDF score.\n",
        "\n",
        "        \"\"\"\n",
        "        self.assertTrue(self.cltfidf.df.loc[\"recognition\"][0]==0)\n",
        "        \n",
        "\n",
        "    def test_forexistinginboth(self):\n",
        "        \"\"\"\n",
        "        For every word existing in both the train and test dataset, they should have a TF-IDF value bigger than 0. This condition is checked.\n",
        "\n",
        "        \"\"\"\n",
        "        for sentences in self.tokenizedtest:\n",
        "          for wordsTest in sentences:\n",
        "            if wordsTest.lower() not in stopwords.words(\"english\") and wordsTest.isalnum():\n",
        "              for sents in self.tokenizedtrain:\n",
        "                for wordsTrain in sents:\n",
        "                  if wordsTrain.lower() not in stopwords.words(\"english\") and wordsTrain.isalnum():\n",
        "                    if wordsTest == wordsTrain:\n",
        "                      self.assertTrue(self.cltfidf.df.loc[wordsTest.lower()][0]>0)\n",
        "          \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "     \n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "OPgzlQrSrfOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''], defaultTest='tfidftest', verbosity=2, exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPRHrOeCrfas",
        "outputId": "ec0a52f7-e246-438f-acaf-38419b25bad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_equalzero (__main__.tfidftest)\n",
            "A non used word in the test set is expected to have a 0 as the TF-IDF score. ... ok\n",
            "test_forexistinginboth (__main__.tfidftest)\n",
            "For every word existing in both the train and test dataset, they should have a TF-IDF value bigger than 0. This condition is checked. ... ok\n",
            "test_similar (__main__.tfidftest)\n",
            "In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone. ... ok\n",
            "test_stopwordsnotexist (__main__.tfidftest)\n",
            "As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. ... ok\n",
            "test_teststr (__main__.tfidftest)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set. ... ok\n",
            "test_trainstr (__main__.tfidftest)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.225s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fdf3282c850>"
            ]
          },
          "metadata": {},
          "execution_count": 279
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class tfidftest_2(unittest.TestCase):\n",
        "\n",
        "    \"\"\"\n",
        "    This is the class that is created for Unit Testing for the TFIDF class created above. In the setUp part, the class with train and test_1  \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "      self.cltfidf=tfidfku(train,test_1)\n",
        "      self.tokenizedtrain=[]\n",
        "      for sents in train:\n",
        "        for sentences in sent_tokenize(sents):\n",
        "            self.tokenizedtrain.append(word_tokenize(sentences))\n",
        "      self.tokenizedtest=[]\n",
        "      for sents in test_1:\n",
        "        for sentences in sent_tokenize(sents):\n",
        "            self.tokenizedtest.append(word_tokenize(sentences))\n",
        "\n",
        "    def test_trainstr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set.\n",
        "        \n",
        "        \"\"\"\n",
        "        for lines in self.cltfidf.train:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_teststr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set.\n",
        "\n",
        "        \"\"\"\n",
        "        for lines in self.cltfidf.test:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "    \n",
        "    def test_stopwordsnotexist(self):\n",
        "        \"\"\"\n",
        "        As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. \n",
        "        \n",
        "        \"\"\"\n",
        "        for words in stopwords.words(\"english\"):\n",
        "            self.assertNotIn(words,self.cltfidf.df.index)\n",
        "\n",
        "    \n",
        "    def test_similar(self):\n",
        "      \"\"\"      \n",
        "      In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone.\n",
        "      So, it is expected that they have equal TF-IDF values. \n",
        "\n",
        "      \"\"\"\n",
        "      self.assertAlmostEqual(self.cltfidf.df.loc[\"machine\"][0],self.cltfidf.df.loc[\"learning\"][0])\n",
        "\n",
        "    def test_equalzero(self):\n",
        "      \"\"\"\n",
        "      A non used word in the test set is expected to have a 0 as the TF-IDF score.\n",
        "\n",
        "      \"\"\"\n",
        "      self.assertTrue(self.cltfidf.df.loc[\"recognition\"][0]==0)\n",
        "\n",
        "    def test_forexistinginboth(self):\n",
        "      \"\"\"\n",
        "      For every word existing in both the train and test dataset, they should have a TF-IDF value bigger than 0. This condition is checked.\n",
        "\n",
        "      \"\"\"\n",
        "      for sentences in self.tokenizedtest:\n",
        "          for wordsTest in sentences:\n",
        "            if wordsTest.lower() not in stopwords.words(\"english\") and wordsTest.isalnum():\n",
        "              for sents in self.tokenizedtrain:\n",
        "                for wordsTrain in sents:\n",
        "                  if wordsTrain.lower() not in stopwords.words(\"english\") and wordsTrain.isalnum():\n",
        "                    if wordsTest == wordsTrain:\n",
        "                      self.assertTrue(self.cltfidf.df.loc[wordsTest.lower()][0]>0)\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "     \n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "13WMwex39Q7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''], defaultTest='tfidftest_2', verbosity=2, exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqVpTNZ59RGd",
        "outputId": "391e7d5e-b2bc-4211-ad0a-bb51444a5615"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_equalzero (__main__.tfidftest_2)\n",
            "A non used word in the test set is expected to have a 0 as the TF-IDF score. ... ok\n",
            "test_forexistinginboth (__main__.tfidftest_2)\n",
            "For every word existing in both the train and test dataset, they should have a TF-IDF value bigger than 0. This condition is checked. ... ok\n",
            "test_similar (__main__.tfidftest_2)\n",
            "In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone. ... ok\n",
            "test_stopwordsnotexist (__main__.tfidftest_2)\n",
            "As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. ... ok\n",
            "test_teststr (__main__.tfidftest_2)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set. ... ok\n",
            "test_trainstr (__main__.tfidftest_2)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 0.647s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fdf327b4850>"
            ]
          },
          "metadata": {},
          "execution_count": 281
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A further work\n",
        "\n",
        "Lemmatization is one of the most common NLP techniques for text preprocessing. With lemmatization, it is possible to reduce a given word to its root. In that way, the TF-IDF algorithm can create more meaningful outputs. For example, it can understand the terms \"gone\" and \"go\" as the same word. This has a significant influence on the TF-IDF scores.SpaCy library's lemmatizer is used for this purpose. The tokens are aimed to be converted to their base forms. "
      ],
      "metadata": {
        "id": "EGTjyVaYO1tQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# lemmatization"
      ],
      "metadata": {
        "id": "zbEV1v49AybO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "# SpaCy natural-language processor for English is created.\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "rknwRnCXhFoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "#Train data and Test data can be anything that has sentences as strings inside. For this experiment, random sentences, including Wikipedia data, are chosen.\n",
        "#Two different Test datasets are used. This shows the different values observed after changing the datasets. The TFIDF values and the terms are changed regarding the test and train datasets.\n",
        "#The first test dataset is a piece of the training dataset.\n",
        "\n",
        "train = [\n",
        "    \"I enjoy reading about Machine Learning and it is my PhD subject.\",\n",
        "    \"I would enjoy a walk in the park.\",\n",
        "    \"I was reading in the library.\",\n",
        "    \"You can not leave the library.\",\n",
        "    \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. \",\n",
        "    \"Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\"\n",
        "\n",
        "]\n",
        "test = [\n",
        "        \"I enjoy reading about Machine Learning and it is my PhD subject.\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "HnO4PUnFV-Fu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train data and Test data can be anything that has sentences as strings inside. For this experiment, random sentences including Wikipedia data are chosen.\n",
        "#Two different Train and Test datasets are used. This shows the different values observed after changing the datasets. The TFIDF values and the terms are changed regarding to the test and train datasets.\n",
        "# The second test dataset does not belong to the training dataset. With the created TF-IDF class, even though new text data that is different than the training data arrives, if there are words existing in both of the datasets, the TF-IDF values are found. \n",
        "#The results are situated below.\n",
        "\n",
        "train= [\n",
        "    \"I enjoy reading about machine Learning and it is my PhD subject.\",\n",
        "    \"I would enjoy a walk in the park.\",\n",
        "    \"I was reading in the library.\",\n",
        "    \"You can not leave the library.\",\n",
        "    \"Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, methods that leverage data to improve performance on some set of tasks. It is seen as a part of artificial intelligence. Machine learning algorithms build a model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so. \",\n",
        "    \"Machine learning algorithms are used in a wide variety of applications, such as in medicine, email filtering, speech recognition, and computer vision, where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.\"\n",
        "\n",
        "]\n",
        "test_1 = [\n",
        "    \"It is a big park. After walking some time in the park, I will go to the library to study Machine Learning. I have to finish my reading before leaving. \"\n",
        "]"
      ],
      "metadata": {
        "id": "VtDERWzxTLHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# class lemmma"
      ],
      "metadata": {
        "id": "UE11cA2ZVzwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class tfidfwlemma():\n",
        "  def __init__(self,train,test) :\n",
        "    self.train=train\n",
        "    self.test=test\n",
        "    self.process()\n",
        "    self.dfresults()\n",
        "\n",
        "  def lemmatize(self,text):\n",
        "      doc = nlp(text)\n",
        "      # The text is turned into its tokens, and the punctuations are ignored\n",
        "      tokens = [token for token in doc if not token.is_punct]\n",
        "      # Those tokens are converted into lemmas by SpaCy.\n",
        "      lemmas = [token.lemma_ if token.pos_ != 'PRON' else token.orth_ for token in tokens]\n",
        "      return lemmas\n",
        "\n",
        "  def process(self):\n",
        "    self.tfidfvectorizer = TfidfVectorizer(stop_words='english',tokenizer=self.lemmatize)\n",
        "    self.tfidfvectorizer.fit(self.train)\n",
        "    #tfidf_train = tfidfvectorizer.transform(train)\n",
        "    self.tfidf_term_vectors  = self.tfidfvectorizer.transform(self.test)\n",
        "\n",
        "  def dfresults(self):\n",
        "    self.df = pd.DataFrame(self.tfidf_term_vectors[0].T.todense(), index=self.tfidfvectorizer.get_feature_names_out(), columns=[\"TF-IDF\"])\n",
        "    self.df = self.df.sort_values('TF-IDF', ascending=False)\n",
        "\n",
        "  def showresults(self):\n",
        "    return self.df\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "TIIWdYbllRmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clw = tfidfwlemma(train,test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9odYuwo0Vknp",
        "outputId": "0079c251-1be7-4edf-e72a-f2fbc8d45976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clw.showresults()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "64L83wFYVkvb",
        "outputId": "a372c531-3c57-4cfd-bb94-e43157b58db4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 TF-IDF\n",
              "subject        0.482050\n",
              "phd            0.482050\n",
              "enjoy          0.395288\n",
              "read           0.395288\n",
              "machine        0.333729\n",
              "learning       0.333729\n",
              "recognition    0.000000\n",
              "model          0.000000\n",
              "need           0.000000\n",
              "order          0.000000\n",
              "park           0.000000\n",
              "perform        0.000000\n",
              "performance    0.000000\n",
              "prediction     0.000000\n",
              "program        0.000000\n",
              "algorithm      0.000000\n",
              "method         0.000000\n",
              "sample         0.000000\n",
              "set            0.000000\n",
              "speech         0.000000\n",
              "task           0.000000\n",
              "training       0.000000\n",
              "understanding  0.000000\n",
              "unfeasible     0.000000\n",
              "use            0.000000\n",
              "variety        0.000000\n",
              "vision         0.000000\n",
              "walk           0.000000\n",
              "ml             0.000000\n",
              "make           0.000000\n",
              "medicine       0.000000\n",
              "application    0.000000\n",
              "artificial     0.000000\n",
              "base           0.000000\n",
              "build          0.000000\n",
              "building       0.000000\n",
              "computer       0.000000\n",
              "conventional   0.000000\n",
              "datum          0.000000\n",
              "decision       0.000000\n",
              "develop        0.000000\n",
              "devote         0.000000\n",
              "difficult      0.000000\n",
              "email          0.000000\n",
              "explicitly     0.000000\n",
              "field          0.000000\n",
              "filtering      0.000000\n",
              "improve        0.000000\n",
              "inquiry        0.000000\n",
              "intelligence   0.000000\n",
              "know           0.000000\n",
              "learn          0.000000\n",
              "leave          0.000000\n",
              "leverage       0.000000\n",
              "library        0.000000\n",
              "wide           0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-156b00af-4f59-4bc7-997a-84f65012e554\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>0.482050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phd</th>\n",
              "      <td>0.482050</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>0.395288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>read</th>\n",
              "      <td>0.395288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>machine</th>\n",
              "      <td>0.333729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.333729</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recognition</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>need</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>park</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perform</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>performance</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prediction</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>program</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algorithm</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>method</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speech</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>task</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>understanding</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unfeasible</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variety</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walk</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ml</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medicine</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>application</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artificial</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>base</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>build</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>building</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computer</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conventional</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>datum</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>develop</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>devote</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>explicitly</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>field</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>filtering</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>improve</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inquiry</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>intelligence</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>know</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learn</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leave</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leverage</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>library</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wide</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-156b00af-4f59-4bc7-997a-84f65012e554')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-156b00af-4f59-4bc7-997a-84f65012e554 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-156b00af-4f59-4bc7-997a-84f65012e554');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 287
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clw_2 = tfidfwlemma(train,test_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cfusn4uFTNtC",
        "outputId": "67304551-fb6d-4b11-c8a4-050405afceb8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clw_2.showresults()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3NNs5sN4TQZb",
        "outputId": "2ebfe73e-afe1-476c-baab-ea41a75ea692"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 TF-IDF\n",
              "park           0.724000\n",
              "walk           0.362000\n",
              "leave          0.362000\n",
              "library        0.296845\n",
              "machine        0.250617\n",
              "learning       0.250617\n",
              "algorithm      0.000000\n",
              "program        0.000000\n",
              "model          0.000000\n",
              "need           0.000000\n",
              "order          0.000000\n",
              "perform        0.000000\n",
              "performance    0.000000\n",
              "phd            0.000000\n",
              "prediction     0.000000\n",
              "read           0.000000\n",
              "method         0.000000\n",
              "recognition    0.000000\n",
              "sample         0.000000\n",
              "set            0.000000\n",
              "speech         0.000000\n",
              "subject        0.000000\n",
              "task           0.000000\n",
              "training       0.000000\n",
              "understanding  0.000000\n",
              "unfeasible     0.000000\n",
              "use            0.000000\n",
              "variety        0.000000\n",
              "vision         0.000000\n",
              "ml             0.000000\n",
              "make           0.000000\n",
              "medicine       0.000000\n",
              "difficult      0.000000\n",
              "artificial     0.000000\n",
              "base           0.000000\n",
              "build          0.000000\n",
              "building       0.000000\n",
              "computer       0.000000\n",
              "conventional   0.000000\n",
              "datum          0.000000\n",
              "decision       0.000000\n",
              "develop        0.000000\n",
              "devote         0.000000\n",
              "email          0.000000\n",
              "application    0.000000\n",
              "enjoy          0.000000\n",
              "explicitly     0.000000\n",
              "field          0.000000\n",
              "filtering      0.000000\n",
              "improve        0.000000\n",
              "inquiry        0.000000\n",
              "intelligence   0.000000\n",
              "know           0.000000\n",
              "learn          0.000000\n",
              "leverage       0.000000\n",
              "wide           0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c5657e96-a062-42f5-b213-43e1cf7e489f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TF-IDF</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>park</th>\n",
              "      <td>0.724000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>walk</th>\n",
              "      <td>0.362000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leave</th>\n",
              "      <td>0.362000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>library</th>\n",
              "      <td>0.296845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>machine</th>\n",
              "      <td>0.250617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learning</th>\n",
              "      <td>0.250617</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>algorithm</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>program</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>model</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>need</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>order</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>perform</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>performance</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>phd</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>prediction</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>read</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>method</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>recognition</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sample</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>set</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>speech</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>subject</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>task</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>training</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>understanding</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unfeasible</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>use</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>variety</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ml</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>make</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>medicine</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>difficult</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>artificial</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>base</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>build</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>building</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>computer</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>conventional</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>datum</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>decision</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>develop</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>devote</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>email</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>application</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>enjoy</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>explicitly</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>field</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>filtering</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>improve</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>inquiry</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>intelligence</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>know</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>learn</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>leverage</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>wide</th>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c5657e96-a062-42f5-b213-43e1cf7e489f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c5657e96-a062-42f5-b213-43e1cf7e489f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c5657e96-a062-42f5-b213-43e1cf7e489f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 289
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a=[]\n",
        "for sents in train:\n",
        "  for sentences in sent_tokenize(sents):\n",
        "      spaword=nlp(sentences)\n",
        "      for words in spaword:\n",
        "        a.append(words.lemma_)\n"
      ],
      "metadata": {
        "id": "njfDK-DIV3kE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy5neb02WYyW",
        "outputId": "beb7502d-6fbe-4dab-c8aa-e117bfcde008"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I',\n",
              " 'enjoy',\n",
              " 'read',\n",
              " 'about',\n",
              " 'machine',\n",
              " 'Learning',\n",
              " 'and',\n",
              " 'it',\n",
              " 'be',\n",
              " 'my',\n",
              " 'phd',\n",
              " 'subject',\n",
              " '.',\n",
              " 'I',\n",
              " 'would',\n",
              " 'enjoy',\n",
              " 'a',\n",
              " 'walk',\n",
              " 'in',\n",
              " 'the',\n",
              " 'park',\n",
              " '.',\n",
              " 'I',\n",
              " 'be',\n",
              " 'read',\n",
              " 'in',\n",
              " 'the',\n",
              " 'library',\n",
              " '.',\n",
              " 'you',\n",
              " 'can',\n",
              " 'not',\n",
              " 'leave',\n",
              " 'the',\n",
              " 'library',\n",
              " '.',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '(',\n",
              " 'ML',\n",
              " ')',\n",
              " 'be',\n",
              " 'a',\n",
              " 'field',\n",
              " 'of',\n",
              " 'inquiry',\n",
              " 'devote',\n",
              " 'to',\n",
              " 'understanding',\n",
              " 'and',\n",
              " 'building',\n",
              " 'method',\n",
              " 'that',\n",
              " \"'\",\n",
              " 'learn',\n",
              " \"'\",\n",
              " ',',\n",
              " 'that',\n",
              " 'is',\n",
              " ',',\n",
              " 'method',\n",
              " 'that',\n",
              " 'leverage',\n",
              " 'datum',\n",
              " 'to',\n",
              " 'improve',\n",
              " 'performance',\n",
              " 'on',\n",
              " 'some',\n",
              " 'set',\n",
              " 'of',\n",
              " 'task',\n",
              " '.',\n",
              " 'it',\n",
              " 'be',\n",
              " 'see',\n",
              " 'as',\n",
              " 'a',\n",
              " 'part',\n",
              " 'of',\n",
              " 'artificial',\n",
              " 'intelligence',\n",
              " '.',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'algorithm',\n",
              " 'build',\n",
              " 'a',\n",
              " 'model',\n",
              " 'base',\n",
              " 'on',\n",
              " 'sample',\n",
              " 'datum',\n",
              " ',',\n",
              " 'know',\n",
              " 'as',\n",
              " 'training',\n",
              " 'datum',\n",
              " ',',\n",
              " 'in',\n",
              " 'order',\n",
              " 'to',\n",
              " 'make',\n",
              " 'prediction',\n",
              " 'or',\n",
              " 'decision',\n",
              " 'without',\n",
              " 'be',\n",
              " 'explicitly',\n",
              " 'program',\n",
              " 'to',\n",
              " 'do',\n",
              " 'so',\n",
              " '.',\n",
              " 'machine',\n",
              " 'learning',\n",
              " 'algorithm',\n",
              " 'be',\n",
              " 'use',\n",
              " 'in',\n",
              " 'a',\n",
              " 'wide',\n",
              " 'variety',\n",
              " 'of',\n",
              " 'application',\n",
              " ',',\n",
              " 'such',\n",
              " 'as',\n",
              " 'in',\n",
              " 'medicine',\n",
              " ',',\n",
              " 'email',\n",
              " 'filtering',\n",
              " ',',\n",
              " 'speech',\n",
              " 'recognition',\n",
              " ',',\n",
              " 'and',\n",
              " 'computer',\n",
              " 'vision',\n",
              " ',',\n",
              " 'where',\n",
              " 'it',\n",
              " 'be',\n",
              " 'difficult',\n",
              " 'or',\n",
              " 'unfeasible',\n",
              " 'to',\n",
              " 'develop',\n",
              " 'conventional',\n",
              " 'algorithm',\n",
              " 'to',\n",
              " 'perform',\n",
              " 'the',\n",
              " 'need',\n",
              " 'task',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 291
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Unit Test"
      ],
      "metadata": {
        "id": "CrL31YGbUCb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Word tokenizer and English stop words are installed to be used in the Unit Tests.\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9nmRHdwUEK6",
        "outputId": "f1a194dc-a04c-4ab9-87a4-1d70e91c1c72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class tfidftestwlemma(unittest.TestCase):\n",
        "\n",
        "    \"\"\"\n",
        "    This is the class created for Unit Testing for the TFIDF class with lemmatization (tfidfwlemma) shown above. \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        In the setUp part, tfidfwlemma class with the train set as \"train\" and test set as \"test\" is created. \n",
        "        The tfidfwlemma class also creates pandas DF thanks to its architecture.\n",
        "        Parameters:\n",
        "        lemmatrain,lemmatest : lemmatized forms of wors in the train and test datasets, respectively.\n",
        "\n",
        "        \"\"\"\n",
        "        self.cltfidf=tfidfwlemma(train,test)\n",
        "       \n",
        "\n",
        "        self.lemmatrain=[]\n",
        "        for sents in train:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              spaword=nlp(sentences)\n",
        "              for words in spaword:\n",
        "                self.lemmatrain.append(words.lemma_)\n",
        "\n",
        "        self.lemmatest=[]\n",
        "        for sents in test:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              spaword=nlp(sentences)\n",
        "              for words in spaword:\n",
        "                self.lemmatest.append(words.lemma_)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def test_trainstr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set.\n",
        "        \"\"\"\n",
        "        \n",
        "        for lines in self.cltfidf.train:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_teststr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set.\n",
        "\n",
        "        \"\"\"\n",
        "        for lines in self.cltfidf.test:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_stopwordsnotexist(self):\n",
        "        \"\"\"\n",
        "        As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. \n",
        "        \n",
        "        \"\"\"\n",
        "        for words in stopwords.words(\"english\"):\n",
        "            self.assertNotIn(words,self.cltfidf.df.index)\n",
        "    \n",
        "    def test_similar(self):\n",
        "        \"\"\"\n",
        "        In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone.\n",
        "        So, it is expected that they have equal TF-IDF values. \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.assertAlmostEqual(self.cltfidf.df.loc[\"machine\"][0],self.cltfidf.df.loc[\"learning\"][0])\n",
        "\n",
        "    def test_equalzero(self):\n",
        "        \"\"\"\n",
        "        A non used word in the test set is expected to have a 0 as the TF-IDF score.\n",
        "\n",
        "        \"\"\"\n",
        "        self.assertTrue(self.cltfidf.df.loc[\"recognition\"][0]==0)\n",
        "        \n",
        "\n",
        "    def test_forexistinginboth(self):\n",
        "        \"\"\"\n",
        "        For the words' lemmatization forms that exist in both the train and test datasets, they should have a TF-IDF value bigger than 0. This condition is checked.\n",
        "\n",
        "        \"\"\"\n",
        "        for words in self.lemmatest:\n",
        "          if words.lower() not in stopwords.words(\"english\") and words.isalnum() and words in self.lemmatrain:\n",
        "            self.assertTrue(self.cltfidf.df.loc[words.lower()][0]>0)\n",
        "\n",
        "\n",
        "        \n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "     \n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "4GXmBTR-A1-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''], defaultTest='tfidftestwlemma', verbosity=2, exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ja-gmS4Vg9x",
        "outputId": "01e6737b-c8e2-46a3-8de7-2ac10acf0f30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_equalzero (__main__.tfidftestwlemma)\n",
            "A non used word in the test set is expected to have a 0 as the TF-IDF score. ... /usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "ok\n",
            "test_forexistinginboth (__main__.tfidftestwlemma)\n",
            "For the words' lemmatization forms that exist in both the train and test datasets, they should have a TF-IDF value bigger than 0. This condition is checked. ... ok\n",
            "test_similar (__main__.tfidftestwlemma)\n",
            "In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone. ... ok\n",
            "test_stopwordsnotexist (__main__.tfidftestwlemma)\n",
            "As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. ... ok\n",
            "test_teststr (__main__.tfidftestwlemma)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set. ... ok\n",
            "test_trainstr (__main__.tfidftestwlemma)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 23.364s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fdf32b53090>"
            ]
          },
          "metadata": {},
          "execution_count": 294
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import unittest\n",
        "\n",
        "class tfidftestwlemma_1(unittest.TestCase):\n",
        "\n",
        "    \"\"\"\n",
        "    This is the class created for Unit Testing for the TFIDF class with lemmatization (tfidfwlemma) shown above. \n",
        "    \n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        In the setUp part, tfidfwlemma class with the train set as \"train\" and test set as \"test_1\" is created. \n",
        "        The tfidfwlemma class also creates pandas DF thanks to its architecture.\n",
        "        Parameters:\n",
        "        lemmatrain,lemmatest : lemmatized forms of wors in the train and test datasets, respectively.\n",
        "\n",
        "        \"\"\"\n",
        "        self.cltfidf=tfidfwlemma(train,test_1)\n",
        "       \n",
        "\n",
        "        self.lemmatrain=[]\n",
        "        for sents in train:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              spaword=nlp(sentences)\n",
        "              for words in spaword:\n",
        "                self.lemmatrain.append(words.lemma_)\n",
        "\n",
        "        self.lemmatest=[]\n",
        "        for sents in test_1:\n",
        "          for sentences in sent_tokenize(sents):\n",
        "              spaword=nlp(sentences)\n",
        "              for words in spaword:\n",
        "                self.lemmatest.append(words.lemma_)\n",
        "        \n",
        "\n",
        "        \n",
        "\n",
        "    def test_trainstr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set.\n",
        "        \"\"\"\n",
        "        \n",
        "        for lines in self.cltfidf.train:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_teststr(self):\n",
        "        \"\"\"\n",
        "        The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set.\n",
        "\n",
        "        \"\"\"\n",
        "        for lines in self.cltfidf.test:\n",
        "         self.assertEqual(lines.__class__.__name__,\"str\")\n",
        "\n",
        "    def test_stopwordsnotexist(self):\n",
        "        \"\"\"\n",
        "        As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. \n",
        "        \n",
        "        \"\"\"\n",
        "        for words in stopwords.words(\"english\"):\n",
        "            self.assertNotIn(words,self.cltfidf.df.index)\n",
        "    \n",
        "    def test_similar(self):\n",
        "        \"\"\"\n",
        "        In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone.\n",
        "        So, it is expected that they have equal TF-IDF values. \n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        self.assertAlmostEqual(self.cltfidf.df.loc[\"machine\"][0],self.cltfidf.df.loc[\"learning\"][0])\n",
        "\n",
        "    def test_equalzero(self):\n",
        "        \"\"\"\n",
        "        A non used word in the test set is expected to have a 0 as the TF-IDF score.\n",
        "\n",
        "        \"\"\"\n",
        "        self.assertTrue(self.cltfidf.df.loc[\"recognition\"][0]==0)\n",
        "        \n",
        "\n",
        "    def test_forexistinginboth(self):\n",
        "        \"\"\"\n",
        "        For the words' lemmatization forms that exist in both the train and test datasets, they should have a TF-IDF value bigger than 0. This condition is checked.\n",
        "\n",
        "        \"\"\"\n",
        "        for words in self.lemmatest:\n",
        "          if words.lower() not in stopwords.words(\"english\") and words.isalnum() and words in self.lemmatrain:\n",
        "            self.assertTrue(self.cltfidf.df.loc[words.lower()][0]>0)\n",
        "\n",
        "\n",
        "        \n",
        "      \n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "     \n",
        "\n",
        "   \n"
      ],
      "metadata": {
        "id": "UiM_fJNdZe3Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unittest.main(argv=[''], defaultTest='tfidftestwlemma_1', verbosity=2, exit=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHqsPcqcZk_n",
        "outputId": "d072b56d-69a6-4ec4-83f5-df82d31819a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "test_equalzero (__main__.tfidftestwlemma_1)\n",
            "A non used word in the test set is expected to have a 0 as the TF-IDF score. ... /usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  \"The parameter 'token_pattern' will not be used\"\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['far', 'make'] not in stop_words.\n",
            "  % sorted(inconsistent)\n",
            "ok\n",
            "test_forexistinginboth (__main__.tfidftestwlemma_1)\n",
            "For the words' lemmatization forms that exist in both the train and test datasets, they should have a TF-IDF value bigger than 0. This condition is checked. ... ok\n",
            "test_similar (__main__.tfidftestwlemma_1)\n",
            "In the test and train datasets, \"Machine Learning\" is used as a bigram. \"Machine\" and \"Learning\" words are not used in any of the datasets alone. ... ok\n",
            "test_stopwordsnotexist (__main__.tfidftestwlemma_1)\n",
            "As the TF-IDF vectorizer uses English stop word removal, the absence of these words is checked in the created TF-IDF DataFrame. ... ok\n",
            "test_teststr (__main__.tfidftestwlemma_1)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the test set. ... ok\n",
            "test_trainstr (__main__.tfidftestwlemma_1)\n",
            "The type of the variables inside of the lists that are used as the inputs for the tfidfku class should be str. This condition is checked for the train set. ... ok\n",
            "\n",
            "----------------------------------------------------------------------\n",
            "Ran 6 tests in 16.631s\n",
            "\n",
            "OK\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<unittest.main.TestProgram at 0x7fdf2e406710>"
            ]
          },
          "metadata": {},
          "execution_count": 296
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "It is observed from the Unit Tests that both of the TF-IDF classes with and w/o Lemmatization worked well. Edge cases are handled as no Error is obtained from the Unit Tests. The results are different whenever new text data that is different from the training data arrives. However, the classes handle it, and they show the existing words TF-IDF in the DF. Moreover, when the results of the classes w/ and w/o lemmatization are compared, it is observed that the results differ. For example, it is noticeable that for the test_1 dataset, there is the word \"walking\" and there is the word \"walk\" in the training dataset. Whenever the lemmatization is applied, the results show that the TF-IDF class with lemmatization takes \"walk\" as a keyword and puts it in the TF-IDF DF with a value higher than 0. If the user does not want particular values about the TF-IDF values of the words, a TF-IDF class with lemmatization as the selected tokenizer could be a better choice. However, it is also important to say that, as the SpaCy is used as the natural-language processor for English, the runtime takes more time for the class w lemmatizing than w/o lemmatizing. So, this issue can create a problem for bigger datasets, and the TF-IDF class w/o lemmatization could be used as the chosen option."
      ],
      "metadata": {
        "id": "PcMgyPO_ZrXy"
      }
    }
  ]
}